{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89da234e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter START date in format 'YYYY-MM-DD': 2000-01-01\n",
      "Enter stock tickers of interest, one at a time, followed by the 'Enter' key (enter q to stop): aapl\n",
      "q\n",
      "Getting AAPL stock data...\n",
      "AAPL stock data gathering complete! :^) \n",
      "Script complete! :D \n"
     ]
    }
   ],
   "source": [
    "#Importing dependencies\n",
    "import datetime as dt #for working with dates\n",
    "import pandas as pd #for working with large lists and large csv files\n",
    "from pandas_datareader import data as pdr #for main stock data\n",
    "import csv #for working with csv files\n",
    "import requests #for html parsing\n",
    "import bs4 #for html parsing\n",
    "from bs4 import BeautifulSoup  #for html parsing\n",
    "import pytrends #for google trends\n",
    "from pytrends.request import TrendReq #for google trends\n",
    "\n",
    "#enter start date, limited by whatever stock has the \"least oldest\" stock data available\n",
    "startDate = input(\"Enter START date in format 'YYYY-MM-DD': \") #get start date from user\n",
    "startDate = dt.datetime.strptime(startDate, '%Y-%m-%d') #make start date a \"datetime\" object in order to work with it\n",
    "#enter end date\n",
    "endDate = dt.datetime.now() #set end date to today (\"datetime\" object)\n",
    "\n",
    "#prompt user for stock tickers\n",
    "userInput = input(\"Enter stock tickers of interest, one at a time, followed by the 'Enter' key (enter q to stop): \")\n",
    "userInput = userInput.upper() #make stock ticker all capital letters\n",
    "\n",
    "stockTickerArray = [] #empty list of stock tickers\n",
    "while (userInput != 'Q'):\n",
    "    try:\n",
    "        currentData = pdr.DataReader(userInput, 'yahoo', startDate, endDate) #try to get main stock data from yahoo finance\n",
    "    except:\n",
    "        print (\"Stock ticker invalid please enter another or press 'q' to stop: \")\n",
    "    else:\n",
    "        stockTickerArray.append(userInput) # add valid stock ticker to list of valid stick tickers\n",
    "        currentFileName = userInput + \"_stock_data\" + \".csv\" #create base file\n",
    "        with open(currentFileName, 'w', encoding= 'UTF8' ) as f: #this writes the current stock tickers data to a csv file\n",
    "            currentData.to_csv(currentFileName) #save data retriebed from yahoo finance (dataframe object) to the base csv\n",
    "            \n",
    "    userInput = input() #get next stock ticker from user\n",
    "    userInput = userInput.upper()\n",
    "\n",
    "    \n",
    "\n",
    "for x in stockTickerArray: #iterate over every stock ticker in array\n",
    "    stock_ticker = x\n",
    "    print(\"Getting\" ,stock_ticker, \"stock data...\" )\n",
    "\n",
    "    fileName = stock_ticker + \"_stock_data.csv\" #current file name is based on the current stock ticker\n",
    "\n",
    "###########################################################################################################################\n",
    "################################################################EPS########################################################\n",
    "\n",
    "    #Macrotrends for past quarter eps data\n",
    "\n",
    "    \n",
    "    URL = \"https://www.macrotrends.net/stocks/charts/\" + stock_ticker + \"/alphabet/eps-earnings-per-share-diluted\"\n",
    "    #get url where eps data is\n",
    "\n",
    "    page = requests.get(URL) #gets HTML from from site using its URL\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\") # this line ensures we use the right parser for HTML\n",
    "    chart = soup.find_all(\"td\") #find all objects in html with attribute \"<td>\"\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    while i < len(chart): # i indexes in range from 0 - (# of elements -1)\n",
    "        if chart[i].text.startswith(('$','2')): #POTENTIALLY desireable data (dates and prices)\n",
    "            if (chart[i].text.isnumeric()): #undesirable data (ex: \"2012\" and its avg (yearly EPS price))\n",
    "                del chart[i+1] #HAVE TO DELETE THE ONE WITH HIGHER INDEX FIRST\n",
    "                del chart[(i)]\n",
    "            else: #desireable data (i.e., quarter dates and quarter prices)\n",
    "                i+=1\n",
    "        else: #once this condition is reached only undesireable data is afterwards due to format of the sites HTMl \n",
    "            while i != len(chart):\n",
    "                del chart[i]\n",
    "    #chart has data of eps quarter dates on even indexes (incl. \"0\") and EPS values on odd values (incl. \"1\")\n",
    "\n",
    "    epsDataList = []\n",
    "    for x in chart:\n",
    "        if (x.text[0] == \"$\"):\n",
    "            epsDataList.append(x.text[1:])\n",
    "        else:\n",
    "            epsDataList.append(x.text)\n",
    "    #epsDataList has eps quarter dates on even indexes (incl. \"0\") and EPS values on odd values (incl. \"1\")\n",
    "    #with most recent data at the beginning\n",
    "\n",
    "    #find earliest and latest (most current) date\n",
    "    earliestEpsDate = dt.datetime.now() #set to random date that is forward in time of earliest EPS date\n",
    "    latestEpsDate = dt.datetime.strptime(\"1900-01-01\", '%Y-%m-%d') #set to random date that is further back in time of latest EPS date\n",
    "    \n",
    "    #actually find correct earliest and latest eps dates\n",
    "    i = 0\n",
    "    while i < (len(epsDataList) - 1 ):\n",
    "        tempDate = dt.datetime.strptime(epsDataList[i], '%Y-%m-%d')\n",
    "        if (tempDate < earliestEpsDate):\n",
    "            earliestEpsDate = tempDate\n",
    "        if tempDate > latestEpsDate:\n",
    "            latestEpsDate = tempDate        \n",
    "        i += 2\n",
    "\n",
    "    #create column to be appended to csv after main data is created\n",
    "    epsColumn = []\n",
    "    i = 0\n",
    "    with open(fileName, 'r', encoding = 'utf-8') as f: #open file\n",
    "        csv_reader = csv.reader(f, delimiter =\",\")\n",
    "        for row in csv_reader: #look at one row at a time\n",
    "            date = row[0]\n",
    "            if (date != \"Date\"): #ignore the header column\n",
    "                fileDate = dt.datetime.strptime(date, '%Y-%m-%d') #convert date in file to datetime object       \n",
    "                if (fileDate < (earliestEpsDate - dt.timedelta(weeks=13))): #check if date in the file is before the earliest quarter we have data for\n",
    "                    epsColumn.append(\"NAN\") \n",
    "                elif (fileDate > latestEpsDate): #check if date in the file is a date that is in the current fiscal quarter\n",
    "                    epsColumn.append(\"NAN\")\n",
    "                else: #we have data for these dates\n",
    "                    while i < (len(epsDataList) - 2 ):\n",
    "                        if (fileDate < earliestEpsDate):\n",
    "                            epsColumn.append(epsDataList[-1]) #end of the list is where the earliest data is stored\n",
    "                            break\n",
    "                        #format of list is \"Date (\\n) Price (\\n) Date (\\n)....(\\n) Price\"\n",
    "                        tempDateRecent = dt.datetime.strptime(epsDataList[i], '%Y-%m-%d')\n",
    "                        tempDatePast = dt.datetime.strptime(epsDataList[i+2], '%Y-%m-%d')\n",
    "                        if (tempDateRecent >= fileDate >= tempDatePast):\n",
    "                            epsColumn.append(epsDataList[i+1])\n",
    "                            if (i != 0):\n",
    "                                i = i-2\n",
    "                            break\n",
    "                        else:\n",
    "                            i += 2\n",
    "                            \n",
    "###########################################################################################################################\n",
    "################################################################INSIDER####################################################\n",
    "  \n",
    "    URL = \"http://openinsider.com/screener?s=\" + \\\n",
    "           stock_ticker + \\\n",
    "           \"&o=&pl=&ph=&ll=&lh=&fd=0&fdr=&td=0&tdr=&\" +             \\\n",
    "           \"fdlyl=&fdlyh=&daysago=&xp=1&xs=1&vl=&vh=&ocl=&och=&sic1=-1&sicl=100&sich=\" + \\\n",
    "           \"9999&grp=0&nfl=&nfh=&nil=&nih=&nol=&noh=&v2l=&v2h=&oc2l=&oc2h=&sortcol=0&cnt=1000&page=1\"\n",
    "    page = requests.get(URL) #gets HTML from from site using its URL\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\") # this line ensures we use the right parser for HTML\n",
    "    table = soup.find(class_=\"tinytable\")\n",
    "    \n",
    "    amt_of_buys = []\n",
    "    amt_of_sells = []\n",
    "    amt_traded = []\n",
    "    try:\n",
    "        data = table.find_all(\"td\")\n",
    "    except:\n",
    "        with open(fileName, 'r', encoding = 'utf-8') as f: #open file\n",
    "            csv_reader = csv.reader(f, delimiter =\",\")\n",
    "            for row in csv_reader: #look at one row at a time\n",
    "                date = row[0]\n",
    "                if (date != \"Date\"): #ignore the header column\n",
    "                    amt_of_buys.append(\"NAN\")\n",
    "                    amt_of_sells.append(\"NAN\")\n",
    "                    amt_traded.append(\"NAN\")\n",
    "        \n",
    "    else:\n",
    "        trade_dates= [] #(Trade date) index 2\n",
    "        titles = [] #(Title) index 5\n",
    "        trade_type = [] #(Trade Type) index 5\n",
    "        transaction = [] #(Qty) index 8\n",
    "        amtOwned= [] #(Owned) index 9\n",
    "\n",
    "        i = 0\n",
    "        while i < len(data):\n",
    "            trade_dates.append(data[i+2].text) \n",
    "            titles.append(data[i+5].text) \n",
    "            trade_type.append(data[i+6].text)\n",
    "            transaction.append(data[i+8].text) \n",
    "            amtOwned.append(data[i+9].text)\n",
    "            i += 16\n",
    "\n",
    "        #Make dates in trade date array into date time objects\n",
    "        i = 0\n",
    "        while i <len(trade_dates):\n",
    "            trade_dates[i] = dt.datetime.strptime(trade_dates[i], '%Y-%m-%d')\n",
    "            i += 1\n",
    "\n",
    "        #remove comma from numbers\n",
    "        i = 0\n",
    "        while i <len(transaction):\n",
    "            transaction[i] = transaction[i].replace(',','')\n",
    "            i += 1\n",
    "            \n",
    "        with open(fileName, 'r', encoding = 'utf-8') as f: #open file\n",
    "            csv_reader = csv.reader(f, delimiter =\",\")\n",
    "            for row in csv_reader: #look at one row at a time\n",
    "                date = row[0]\n",
    "                if (date != \"Date\"): #ignore the header column\n",
    "                    fileDate = dt.datetime.strptime(date, '%Y-%m-%d')#convert date in file to datetime object\n",
    "                    if (fileDate < trade_dates[-1]): #if date in file is less than earliest date\n",
    "                        amt_of_buys.append(\"NAN\")\n",
    "                        amt_of_sells.append(\"NAN\")\n",
    "                        amt_traded.append(\"NAN\")\n",
    "                    elif (fileDate > trade_dates[0]): #if date in file is more recent than latest date\n",
    "                        amt_of_buys.append(\"NAN\")\n",
    "                        amt_of_sells.append(\"NAN\")\n",
    "                        amt_traded.append(\"NAN\")\n",
    "                    else:\n",
    "                        try:\n",
    "                            index = trade_dates.index(fileDate) #find at what index is the file date in the trade dates list\n",
    "                        except ValueError: #no reported trades on this day\n",
    "                            amt_of_buys.append(0)\n",
    "                            amt_of_sells.append(0)\n",
    "                            amt_traded.append(0)\n",
    "                        else:\n",
    "                            #If the date from csv is found in trade_dates list\n",
    "                            transactionCounter = 0\n",
    "                            buyCounter = 0\n",
    "                            sellCounter = 0\n",
    "                            #count all buys, sells, and total amount traded\n",
    "                            while index < len(trade_dates):\n",
    "                                if (trade_dates[index] == fileDate):\n",
    "                                    transactionCounter += int(transaction[index])\n",
    "                                    if (trade_type[index] == \"P - Purchase\"):\n",
    "                                        sellCounter += 1\n",
    "                                    else:\n",
    "                                        buyCounter += 1\n",
    "                                else:\n",
    "                                    break\n",
    "                                index += 1\n",
    "                            amt_of_buys.append(buyCounter)\n",
    "                            amt_of_sells.append(sellCounter)\n",
    "                            amt_traded.append(transactionCounter)\n",
    "    \n",
    "###########################################################################################################################\n",
    "############################################################INFLATION######################################################\n",
    "    \n",
    "    # base year = str(startDate.year)\n",
    "    # $1 in (base_year) is roughly equivalent (in purchasing power) to (inflated amount) in (whatever year you are looking at)\n",
    "\n",
    "    url = \"https://www.officialdata.org/us/inflation/\" + str(startDate.year) + \"?amount=1#buying-power\"\n",
    "    page = requests.get(url) #gets HTML from from site using its URL\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\") # this line ensures we use the right parser for HTML\\\n",
    "    table = soup.find(class_ = \"regular-data table-striped\") #find table with desired data (html class = ...)\n",
    "    data = table.find_all(\"tr\") #find all elements wrapped in <tr> tag in table\n",
    "    \n",
    "    #for x in data:\n",
    "    #    print(x)\n",
    "    #    print(x.text[0])\n",
    "    #    print(x.text.splitlines())\n",
    "    #    print(x.text.splitlines()[0][0])\n",
    "\n",
    "    inflationDataList = []\n",
    "\n",
    "\n",
    "    #get data from chart\n",
    "    for x in data:\n",
    "        if x.text[0] != 'Y':\n",
    "            year = int(x.text[0] + x.text[1] + x.text[2] + x.text[3])\n",
    "            dollar_value = (x.text[5] + x.text[6] + x.text[7] + x.text[8])\n",
    "            inflation_rate = (x.text[9] + x.text[10] + x.text[11] + x.text[12] + x.text[13])\n",
    "            inflationDataList.extend((year, dollar_value, inflation_rate))\n",
    "        \n",
    "        \n",
    "    #get data from chart\n",
    "    #for x in data:\n",
    "    #    if x.text.splitlines()[1] != \"Year\":\n",
    "    #        year = int(x.text.splitlines()[1])\n",
    "    #        dollar_value = x.text.splitlines()[2]\n",
    "    #        inflation_rate = x.text.splitlines()[3]\n",
    "    #        inflationDataList.extend((year, dollar_value, inflation_rate))\n",
    "\n",
    "    inflationColumn = []  \n",
    "    i=0\n",
    "    with open(fileName, 'r', encoding = 'utf-8') as f: #open file\n",
    "        csv_reader = csv.reader(f, delimiter =\",\")\n",
    "        for row in csv_reader: #look at one row at a time\n",
    "            date = row[0]        \n",
    "            if (date != \"Date\"): #ignore the header column\n",
    "                fileDate = dt.datetime.strptime(date, '%Y-%m-%d')#convert date in file to datetime object\n",
    "                while i *3 < (len(inflationDataList) - 1):\n",
    "                    if int(fileDate.year) == inflationDataList[3*i]:\n",
    "                        inflationColumn.append(inflationDataList[(3*i) + 2])\n",
    "                        break\n",
    "                    else:\n",
    "                        i += 1\n",
    "                        \n",
    "###########################################################################################################################\n",
    "##################################################GOOGLE TREND#############################################################\n",
    "\n",
    "    pytrends = TrendReq(hl='en-US', tz=360) #connect to google\n",
    "    kw_list = [stock_ticker] #list of keywords (doing one at a time) (may  generate 409 error if too many tickers)\n",
    "    \n",
    "    stock_df = pd.read_csv(fileName) \n",
    "    trend_start_date = stock_df.iloc[0,0] #get earliest stock data date \n",
    "    trend_end_date =stock_df.iloc[-1,0] #get most recent stock data date \n",
    "    trend_time_frame = \"2004-01-01\" + \" \" + trend_end_date\n",
    "    \n",
    "    pytrends.build_payload(kw_list, cat=0, timeframe= trend_time_frame, geo='', gprop='') #build payload\n",
    "    data = pytrends.interest_over_time() #put data in df object\n",
    "    \n",
    "    i = 0\n",
    "    gtrendData = []\n",
    "    with open(fileName, 'r', encoding = 'utf-8') as f: #open file\n",
    "        csv_reader = csv.reader(f, delimiter =\",\")\n",
    "        for row in csv_reader: #look at one row at a time\n",
    "            date = row[0]\n",
    "            if (date != \"Date\"): #ignore the header column\n",
    "                fileDate = dt.datetime.strptime(date, '%Y-%m-%d') #convert date in file to datetime object       \n",
    "                if (fileDate < data.index[0]): #check if date in the file is before the earliest date we have data for\n",
    "                    gtrendData.append(\"NAN\") \n",
    "                elif (fileDate > data.index[-1]): #check if date in the file is a date that is too recent\n",
    "                    gtrendData.append(\"NAN\")\n",
    "                    #print(\"file date = \", fileDate, \"trend date = \", data.index[-1])\n",
    "                else: #we have data for these dates\n",
    "                    while i < (len(data)):\n",
    "                        if (data.index[i].month == fileDate.month):\n",
    "                            gtrendData.append(data.iloc[i,0])\n",
    "                            break\n",
    "                        else:\n",
    "                            i += 1\n",
    "                            \n",
    "        \n",
    "        \n",
    "    #print(gtrendData)\n",
    "                            \n",
    "###########################################################################################################################\n",
    "#################################################APPEND EVERYTHING#########################################################\n",
    "    \n",
    "    df = pd.read_csv(fileName)\n",
    "    buys_column = pd.DataFrame({'Buys': amt_of_buys})\n",
    "    sells_column = pd.DataFrame({'Sells': amt_of_sells})\n",
    "    traded_column = pd.DataFrame({'Traded': amt_traded})\n",
    "    eps_column = pd.DataFrame({'EPS': epsColumn})\n",
    "    inflation_column = pd.DataFrame({'IR': inflationColumn})\n",
    "    gt_column = pd.DataFrame({'GT': gtrendData})\n",
    "    df = df.merge(buys_column, left_index = True, right_index = True)\n",
    "    df = df.merge(sells_column, left_index = True, right_index = True)\n",
    "    df = df.merge(traded_column, left_index = True, right_index = True)\n",
    "    df = df.merge(eps_column, left_index = True, right_index = True)\n",
    "    df = df.merge(inflation_column, left_index = True, right_index = True)\n",
    "    df = df.merge(gt_column, left_index = True, right_index = True)\n",
    "    df.to_csv(fileName, index = False)\n",
    "    print(stock_ticker, \"stock data gathering complete! :^) \")\n",
    "\n",
    "############################################################################################################################\n",
    "print(\"Script complete! :D \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433961ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da70a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
